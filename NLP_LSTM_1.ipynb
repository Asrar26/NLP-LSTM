{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> **libraries for data handling**\n",
        "\n"
      ],
      "metadata": {
        "id": "5ZoClm_6hZC9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import os \n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ],
      "metadata": {
        "id": "LkBhMFcxwesa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.framework.random_seed import set_random_seed"
      ],
      "metadata": {
        "id": "9sUyajMYiZ1a"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set seeds for reproducability\n",
        "from numpy.random import seed\n",
        "set_random_seed(2)\n",
        "seed(1)\n",
        "# keras module for building LSTM \n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Sequential\n",
        "import keras.utils as ku"
      ],
      "metadata": {
        "id": "rKlAI_AOhhSO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Loading the Dataset**\n",
        "\n"
      ],
      "metadata": {
        "id": "6VhpFwdyirdy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kwm0g-4pG51",
        "outputId": "18a06da3-b8cd-4a2b-892f-834ca209a430"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.8/dist-packages (1.5.12)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from kaggle) (2022.12.7)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.8/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from kaggle) (2.25.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from kaggle) (4.64.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.8/dist-packages (from kaggle) (7.0.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.8/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.8/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.8/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->kaggle) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->kaggle) (2.10)\n"
          ]
        }
      ],
      "source": [
        "! pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -q kaggle"
      ],
      "metadata": {
        "id": "1ccaDqVya-UK"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "fhKi0Noh302j",
        "outputId": "5772697c-b808-435f-dc6d-523ceb5edbe4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b165a774-5999-4bcc-a5ca-6e4d7a41945d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b165a774-5999-4bcc-a5ca-6e4d7a41945d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle (5).json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"asrargalt\",\"key\":\"5ed745b7c3591129b660816a5d292acf\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! cp kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "1_CSJcR3wD6O"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "U8VmQNaYwKss"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle datasets download -d aashita/nyt-comments"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LOAvRfTwOW6",
        "outputId": "c8bc669b-4c21-484d-a55c-8bf2acf85c9b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nyt-comments.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip /content/nyt-comments.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVV_CS2FwVd_",
        "outputId": "e2e71aca-025d-4086-e80f-d56dac1ec6df"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/nyt-comments.zip\n",
            "replace ArticlesApril2017.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: ArticlesApril2017.csv   \n",
            "  inflating: ArticlesApril2018.csv   \n",
            "  inflating: ArticlesFeb2017.csv     \n",
            "  inflating: ArticlesFeb2018.csv     \n",
            "  inflating: ArticlesJan2017.csv     \n",
            "  inflating: ArticlesJan2018.csv     \n",
            "  inflating: ArticlesMarch2017.csv   \n",
            "  inflating: ArticlesMarch2018.csv   \n",
            "  inflating: ArticlesMay2017.csv     \n",
            "  inflating: CommentsApril2017.csv   \n",
            "  inflating: CommentsApril2018.csv   \n",
            "  inflating: CommentsFeb2017.csv     \n",
            "  inflating: CommentsFeb2018.csv     \n",
            "  inflating: CommentsJan2017.csv     \n",
            "  inflating: CommentsJan2018.csv     \n",
            "  inflating: CommentsMarch2017.csv   \n",
            "  inflating: CommentsMarch2018.csv   \n",
            "  inflating: CommentsMay2017.csv     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read Articals headlines"
      ],
      "metadata": {
        "id": "-nyE2c7MguZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "curr_dir = '/content/'\n",
        "all_headlines = [] \n",
        "for filename in os.listdir(curr_dir):\n",
        " if 'Articles' in filename:\n",
        "    article_df = pd.read_csv(curr_dir + filename) \n",
        "    all_headlines.extend(list(article_df.headline.values))\n",
        "    break \n",
        "all_headlines = [h for h in all_headlines if h != \"Unknown\"] \n",
        "len(all_headlines)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pZ0g1hrwqfy",
        "outputId": "24e1468d-95df-4c37-eb7f-9add1f40fd3b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "829"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_headlines = [line for line in all_headlines if line!= \"Unknown\"]\n",
        "print(all_headlines[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uA-ol9czk8cI",
        "outputId": "76e30417-d025-4ebe-fcc4-00431de23c27"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['N.F.L. vs. Politics Has Been Battle All Season Long', 'Voice. Vice. Veracity.', 'A Stand-Up’s Downward Slide', 'New York Today: A Groundhog Has Her Day', 'A Swimmer’s Communion With the Ocean', 'Trail Activity', 'Super Bowl', 'Trump’s Mexican Shakedown', 'Pence’s Presidential Pet', 'Fruit of a Poison Tree']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Cleaning:"
      ],
      "metadata": {
        "id": "2_IEihZsg4Hy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_data(headlines):\n",
        "    headlines = \"\".join(i for i in headlines if i not in string.punctuation).lower()\n",
        "    headlines = headlines.encode(\"utf8\").decode(\"ascii\",'ignore')\n",
        "    return headlines \n",
        "data = [clean_data(x) for x in all_headlines]"
      ],
      "metadata": {
        "id": "GL_h_HSWg3MN"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[0:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMcrTOM0gKWU",
        "outputId": "f6f46bb1-957c-43c5-9162-f6bc9924e2c4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['nfl vs politics has been battle all season long',\n",
              " 'voice vice veracity',\n",
              " 'a standups downward slide',\n",
              " 'new york today a groundhog has her day',\n",
              " 'a swimmers communion with the ocean',\n",
              " 'trail activity',\n",
              " 'super bowl',\n",
              " 'trumps mexican shakedown',\n",
              " 'pences presidential pet',\n",
              " 'fruit of a poison tree']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2) Generating Sequence of N-Grams Tokens:"
      ],
      "metadata": {
        "id": "s3iiuNtYicQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "Mkj15qSaiVRe"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()"
      ],
      "metadata": {
        "id": "uckrees2jBnG"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sequence_of_tokens(data):\n",
        "    tokenizer.fit_on_texts(data)\n",
        "    words = len(tokenizer.word_index)+1\n",
        "    input_sequences = []\n",
        "    for line in data:\n",
        "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "        for i in range(1,len(token_list)):\n",
        "            n_gram_sequence = token_list[:i+1]\n",
        "            input_sequences.append(n_gram_sequence)\n",
        "    return input_sequences,words"
      ],
      "metadata": {
        "id": "3Yods5WDi6Uq"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences,words = get_sequence_of_tokens(data)"
      ],
      "metadata": {
        "id": "-QETqgmujU8s"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences[0:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GloBZIcujW-_",
        "outputId": "64a221eb-d4e8-4492-c15f-2d2d3785b8cf"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[660, 117],\n",
              " [660, 117, 72],\n",
              " [660, 117, 72, 73],\n",
              " [660, 117, 72, 73, 661],\n",
              " [660, 117, 72, 73, 661, 662],\n",
              " [660, 117, 72, 73, 661, 662, 63],\n",
              " [660, 117, 72, 73, 661, 662, 63, 29],\n",
              " [660, 117, 72, 73, 661, 662, 63, 29, 210],\n",
              " [211, 663],\n",
              " [211, 663, 664]]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3) Padding the Sequences:"
      ],
      "metadata": {
        "id": "5BDiBiXNjk90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras_preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "dNrVEz-vjbw_"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_sequence_len = max([len(x) for x in sequences])\n",
        "input_sequences = np.array(pad_sequences(sequences,maxlen=max_sequence_len,padding='pre'))"
      ],
      "metadata": {
        "id": "91XSnaa_jwo9"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4) Creating Predictors and Targets:"
      ],
      "metadata": {
        "id": "1in8i0B2nFjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictors,label = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "label = tf.keras.utils.to_categorical(label,num_classes=words)"
      ],
      "metadata": {
        "id": "GRlfXSTImiXt"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_len = max_sequence_len - 1\n",
        "model = tf.keras.Sequential()\n",
        "# ----------Add Input Embedding Layer\n",
        "model.add(tf.keras.layers.Embedding(words,50,input_length=input_len))\n",
        "# ----------Add Hidden Layer 1 - LSTM Layer\n",
        "model.add(tf.keras.layers.LSTM(500))\n",
        "model.add(Dropout(0.1))\n",
        "# ----------Add Output Layer\n",
        "model.add(tf.keras.layers.Dense(words,activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam')\n",
        "model = create_model(max_sequence_len, words)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcL5dTdNms5z",
        "outputId": "69ac126a-0d20-4a56-e255-0e1cf0d3530a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 16, 10)            22880     \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 100)               44400     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 100)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2288)              231088    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 298,368\n",
            "Trainable params: 298,368\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> **Training the model**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RknFuVL0pB7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(predictors,label,epochs=800)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Om44jyJApHpD",
        "outputId": "92123086-2495-44a7-abf8-4488ea7ca2a0"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/800\n",
            "142/142 [==============================] - 4s 5ms/step - loss: 7.3454\n",
            "Epoch 2/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 6.8410\n",
            "Epoch 3/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 6.7241\n",
            "Epoch 4/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 6.6409\n",
            "Epoch 5/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 6.5604\n",
            "Epoch 6/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 6.4766\n",
            "Epoch 7/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 6.3827\n",
            "Epoch 8/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 6.2885\n",
            "Epoch 9/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 6.1882\n",
            "Epoch 10/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 6.0893\n",
            "Epoch 11/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 5.9946\n",
            "Epoch 12/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 5.8996\n",
            "Epoch 13/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 5.8118\n",
            "Epoch 14/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 5.7229\n",
            "Epoch 15/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 5.6311\n",
            "Epoch 16/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 5.5422\n",
            "Epoch 17/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 5.4533\n",
            "Epoch 18/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 5.3642\n",
            "Epoch 19/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 5.2765\n",
            "Epoch 20/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 5.1948\n",
            "Epoch 21/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 5.1107\n",
            "Epoch 22/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 5.0271\n",
            "Epoch 23/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 4.9496\n",
            "Epoch 24/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 4.8730\n",
            "Epoch 25/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 4.7893\n",
            "Epoch 26/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 4.7164\n",
            "Epoch 27/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 4.6383\n",
            "Epoch 28/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 4.5583\n",
            "Epoch 29/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 4.4812\n",
            "Epoch 30/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 4.4015\n",
            "Epoch 31/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 4.3262\n",
            "Epoch 32/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 4.2532\n",
            "Epoch 33/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 4.1897\n",
            "Epoch 34/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 4.1086\n",
            "Epoch 35/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 4.0457\n",
            "Epoch 36/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 3.9771\n",
            "Epoch 37/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 3.9067\n",
            "Epoch 38/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 3.8400\n",
            "Epoch 39/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 3.7845\n",
            "Epoch 40/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 3.7175\n",
            "Epoch 41/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 3.6591\n",
            "Epoch 42/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 3.5978\n",
            "Epoch 43/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 3.5380\n",
            "Epoch 44/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 3.4861\n",
            "Epoch 45/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 3.4201\n",
            "Epoch 46/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 3.3750\n",
            "Epoch 47/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 3.3154\n",
            "Epoch 48/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 3.2668\n",
            "Epoch 49/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 3.2183\n",
            "Epoch 50/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 3.1659\n",
            "Epoch 51/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 3.1191\n",
            "Epoch 52/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 3.0678\n",
            "Epoch 53/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 3.0210\n",
            "Epoch 54/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 2.9690\n",
            "Epoch 55/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 2.9389\n",
            "Epoch 56/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 2.8905\n",
            "Epoch 57/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 2.8491\n",
            "Epoch 58/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 2.8113\n",
            "Epoch 59/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 2.7616\n",
            "Epoch 60/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 2.7173\n",
            "Epoch 61/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 2.6796\n",
            "Epoch 62/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 2.6465\n",
            "Epoch 63/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 2.6075\n",
            "Epoch 64/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 2.5716\n",
            "Epoch 65/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 2.5358\n",
            "Epoch 66/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 2.5011\n",
            "Epoch 67/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 2.4580\n",
            "Epoch 68/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 2.4431\n",
            "Epoch 69/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 2.4042\n",
            "Epoch 70/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 2.3707\n",
            "Epoch 71/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 2.3369\n",
            "Epoch 72/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 2.3060\n",
            "Epoch 73/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 2.2840\n",
            "Epoch 74/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 2.2405\n",
            "Epoch 75/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 2.2099\n",
            "Epoch 76/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 2.1783\n",
            "Epoch 77/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 2.1687\n",
            "Epoch 78/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 2.1214\n",
            "Epoch 79/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 2.1062\n",
            "Epoch 80/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 2.0854\n",
            "Epoch 81/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 2.0503\n",
            "Epoch 82/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 2.0262\n",
            "Epoch 83/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.9897\n",
            "Epoch 84/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.9667\n",
            "Epoch 85/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.9493\n",
            "Epoch 86/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.9272\n",
            "Epoch 87/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.9022\n",
            "Epoch 88/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.8855\n",
            "Epoch 89/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.8498\n",
            "Epoch 90/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.8318\n",
            "Epoch 91/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.8141\n",
            "Epoch 92/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.7944\n",
            "Epoch 93/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.7682\n",
            "Epoch 94/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.7382\n",
            "Epoch 95/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.7281\n",
            "Epoch 96/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.7110\n",
            "Epoch 97/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.6997\n",
            "Epoch 98/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.6696\n",
            "Epoch 99/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.6380\n",
            "Epoch 100/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.6298\n",
            "Epoch 101/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.6022\n",
            "Epoch 102/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.5934\n",
            "Epoch 103/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.5661\n",
            "Epoch 104/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.5547\n",
            "Epoch 105/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.5354\n",
            "Epoch 106/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.5166\n",
            "Epoch 107/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.5017\n",
            "Epoch 108/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.4806\n",
            "Epoch 109/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.4606\n",
            "Epoch 110/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.4480\n",
            "Epoch 111/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.4453\n",
            "Epoch 112/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.4288\n",
            "Epoch 113/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.4006\n",
            "Epoch 114/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.3834\n",
            "Epoch 115/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.3866\n",
            "Epoch 116/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.3481\n",
            "Epoch 117/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.3481\n",
            "Epoch 118/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.3419\n",
            "Epoch 119/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.3171\n",
            "Epoch 120/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.2958\n",
            "Epoch 121/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.2863\n",
            "Epoch 122/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.2633\n",
            "Epoch 123/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.2689\n",
            "Epoch 124/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.2465\n",
            "Epoch 125/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.2260\n",
            "Epoch 126/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.2208\n",
            "Epoch 127/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.2040\n",
            "Epoch 128/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.1928\n",
            "Epoch 129/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.1835\n",
            "Epoch 130/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.1662\n",
            "Epoch 131/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.1536\n",
            "Epoch 132/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.1499\n",
            "Epoch 133/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.1352\n",
            "Epoch 134/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.1166\n",
            "Epoch 135/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.1061\n",
            "Epoch 136/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.0973\n",
            "Epoch 137/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.0768\n",
            "Epoch 138/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.0705\n",
            "Epoch 139/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.0717\n",
            "Epoch 140/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.0444\n",
            "Epoch 141/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.0485\n",
            "Epoch 142/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.0293\n",
            "Epoch 143/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.0305\n",
            "Epoch 144/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.0177\n",
            "Epoch 145/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 1.0023\n",
            "Epoch 146/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.9921\n",
            "Epoch 147/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.9769\n",
            "Epoch 148/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.9696\n",
            "Epoch 149/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.9619\n",
            "Epoch 150/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.9488\n",
            "Epoch 151/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.9436\n",
            "Epoch 152/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.9323\n",
            "Epoch 153/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.9233\n",
            "Epoch 154/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.9161\n",
            "Epoch 155/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.9023\n",
            "Epoch 156/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.8877\n",
            "Epoch 157/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.8840\n",
            "Epoch 158/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.8843\n",
            "Epoch 159/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.8752\n",
            "Epoch 160/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.8624\n",
            "Epoch 161/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.8534\n",
            "Epoch 162/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.8451\n",
            "Epoch 163/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.8508\n",
            "Epoch 164/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.8274\n",
            "Epoch 165/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.8178\n",
            "Epoch 166/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.8093\n",
            "Epoch 167/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.8172\n",
            "Epoch 168/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.8113\n",
            "Epoch 169/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.7978\n",
            "Epoch 170/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.7878\n",
            "Epoch 171/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.7733\n",
            "Epoch 172/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.7674\n",
            "Epoch 173/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.7683\n",
            "Epoch 174/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.7543\n",
            "Epoch 175/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.7429\n",
            "Epoch 176/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.7418\n",
            "Epoch 177/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.7395\n",
            "Epoch 178/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.7423\n",
            "Epoch 179/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.7260\n",
            "Epoch 180/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.7217\n",
            "Epoch 181/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.7161\n",
            "Epoch 182/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.7062\n",
            "Epoch 183/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.7067\n",
            "Epoch 184/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.7003\n",
            "Epoch 185/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.6908\n",
            "Epoch 186/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.6810\n",
            "Epoch 187/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.6752\n",
            "Epoch 188/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.6744\n",
            "Epoch 189/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.6698\n",
            "Epoch 190/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.6541\n",
            "Epoch 191/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.6548\n",
            "Epoch 192/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.6487\n",
            "Epoch 193/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.6445\n",
            "Epoch 194/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.6314\n",
            "Epoch 195/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.6299\n",
            "Epoch 196/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.6244\n",
            "Epoch 197/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.6247\n",
            "Epoch 198/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.6233\n",
            "Epoch 199/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.6177\n",
            "Epoch 200/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.6088\n",
            "Epoch 201/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.6061\n",
            "Epoch 202/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.6080\n",
            "Epoch 203/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.5990\n",
            "Epoch 204/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.5931\n",
            "Epoch 205/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.5927\n",
            "Epoch 206/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.5852\n",
            "Epoch 207/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.5818\n",
            "Epoch 208/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.5861\n",
            "Epoch 209/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.5683\n",
            "Epoch 210/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.5683\n",
            "Epoch 211/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.5667\n",
            "Epoch 212/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.5612\n",
            "Epoch 213/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.5629\n",
            "Epoch 214/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.5514\n",
            "Epoch 215/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.5513\n",
            "Epoch 216/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.5416\n",
            "Epoch 217/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.5374\n",
            "Epoch 218/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.5377\n",
            "Epoch 219/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.5433\n",
            "Epoch 220/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.5392\n",
            "Epoch 221/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.5251\n",
            "Epoch 222/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.5338\n",
            "Epoch 223/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.5129\n",
            "Epoch 224/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.5243\n",
            "Epoch 225/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.5033\n",
            "Epoch 226/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.5162\n",
            "Epoch 227/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.5197\n",
            "Epoch 228/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.5075\n",
            "Epoch 229/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.5122\n",
            "Epoch 230/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.5101\n",
            "Epoch 231/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.4957\n",
            "Epoch 232/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.4976\n",
            "Epoch 233/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.4881\n",
            "Epoch 234/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.4806\n",
            "Epoch 235/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.4967\n",
            "Epoch 236/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.4790\n",
            "Epoch 237/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.4808\n",
            "Epoch 238/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.4760\n",
            "Epoch 239/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.4704\n",
            "Epoch 240/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.4697\n",
            "Epoch 241/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.4629\n",
            "Epoch 242/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.4682\n",
            "Epoch 243/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.4710\n",
            "Epoch 244/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.4692\n",
            "Epoch 245/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.4643\n",
            "Epoch 246/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.4618\n",
            "Epoch 247/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.4530\n",
            "Epoch 248/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.4491\n",
            "Epoch 249/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.4524\n",
            "Epoch 250/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.4507\n",
            "Epoch 251/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.4492\n",
            "Epoch 252/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.4456\n",
            "Epoch 253/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.4515\n",
            "Epoch 254/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.4408\n",
            "Epoch 255/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.4494\n",
            "Epoch 256/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.4421\n",
            "Epoch 257/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.4430\n",
            "Epoch 258/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.4338\n",
            "Epoch 259/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.4360\n",
            "Epoch 260/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.4250\n",
            "Epoch 261/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.4312\n",
            "Epoch 262/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.4355\n",
            "Epoch 263/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.4214\n",
            "Epoch 264/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.4340\n",
            "Epoch 265/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.4138\n",
            "Epoch 266/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.4247\n",
            "Epoch 267/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.4175\n",
            "Epoch 268/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.4181\n",
            "Epoch 269/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.4165\n",
            "Epoch 270/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.4128\n",
            "Epoch 271/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.4193\n",
            "Epoch 272/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.4194\n",
            "Epoch 273/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.4189\n",
            "Epoch 274/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.4147\n",
            "Epoch 275/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.4101\n",
            "Epoch 276/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.4011\n",
            "Epoch 277/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.4018\n",
            "Epoch 278/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3965\n",
            "Epoch 279/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.4074\n",
            "Epoch 280/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.4025\n",
            "Epoch 281/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.4043\n",
            "Epoch 282/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3984\n",
            "Epoch 283/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.4059\n",
            "Epoch 284/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.4105\n",
            "Epoch 285/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3943\n",
            "Epoch 286/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3948\n",
            "Epoch 287/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3921\n",
            "Epoch 288/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3836\n",
            "Epoch 289/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3892\n",
            "Epoch 290/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3886\n",
            "Epoch 291/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3883\n",
            "Epoch 292/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3971\n",
            "Epoch 293/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3790\n",
            "Epoch 294/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3853\n",
            "Epoch 295/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3818\n",
            "Epoch 296/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3872\n",
            "Epoch 297/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3876\n",
            "Epoch 298/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3785\n",
            "Epoch 299/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3877\n",
            "Epoch 300/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3853\n",
            "Epoch 301/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3809\n",
            "Epoch 302/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3812\n",
            "Epoch 303/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3832\n",
            "Epoch 304/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3683\n",
            "Epoch 305/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3750\n",
            "Epoch 306/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3727\n",
            "Epoch 307/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3769\n",
            "Epoch 308/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3684\n",
            "Epoch 309/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3732\n",
            "Epoch 310/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3669\n",
            "Epoch 311/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3725\n",
            "Epoch 312/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3684\n",
            "Epoch 313/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3691\n",
            "Epoch 314/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3668\n",
            "Epoch 315/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3691\n",
            "Epoch 316/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3623\n",
            "Epoch 317/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3704\n",
            "Epoch 318/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3654\n",
            "Epoch 319/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3658\n",
            "Epoch 320/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3616\n",
            "Epoch 321/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3607\n",
            "Epoch 322/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3659\n",
            "Epoch 323/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3646\n",
            "Epoch 324/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3657\n",
            "Epoch 325/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3604\n",
            "Epoch 326/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3652\n",
            "Epoch 327/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3598\n",
            "Epoch 328/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3558\n",
            "Epoch 329/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3575\n",
            "Epoch 330/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3608\n",
            "Epoch 331/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3569\n",
            "Epoch 332/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3513\n",
            "Epoch 333/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3666\n",
            "Epoch 334/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3559\n",
            "Epoch 335/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3597\n",
            "Epoch 336/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3528\n",
            "Epoch 337/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3555\n",
            "Epoch 338/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3530\n",
            "Epoch 339/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3507\n",
            "Epoch 340/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3487\n",
            "Epoch 341/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3434\n",
            "Epoch 342/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3487\n",
            "Epoch 343/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3419\n",
            "Epoch 344/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3490\n",
            "Epoch 345/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3585\n",
            "Epoch 346/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3494\n",
            "Epoch 347/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3471\n",
            "Epoch 348/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3398\n",
            "Epoch 349/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3445\n",
            "Epoch 350/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3515\n",
            "Epoch 351/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3490\n",
            "Epoch 352/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3411\n",
            "Epoch 353/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3451\n",
            "Epoch 354/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3513\n",
            "Epoch 355/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3421\n",
            "Epoch 356/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3446\n",
            "Epoch 357/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3557\n",
            "Epoch 358/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3503\n",
            "Epoch 359/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3382\n",
            "Epoch 360/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3439\n",
            "Epoch 361/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3412\n",
            "Epoch 362/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3445\n",
            "Epoch 363/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3389\n",
            "Epoch 364/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3401\n",
            "Epoch 365/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3352\n",
            "Epoch 366/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3416\n",
            "Epoch 367/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3433\n",
            "Epoch 368/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3387\n",
            "Epoch 369/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3396\n",
            "Epoch 370/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3381\n",
            "Epoch 371/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3315\n",
            "Epoch 372/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3433\n",
            "Epoch 373/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3261\n",
            "Epoch 374/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3365\n",
            "Epoch 375/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3344\n",
            "Epoch 376/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3330\n",
            "Epoch 377/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3365\n",
            "Epoch 378/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3304\n",
            "Epoch 379/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3339\n",
            "Epoch 380/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3464\n",
            "Epoch 381/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3414\n",
            "Epoch 382/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3377\n",
            "Epoch 383/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3356\n",
            "Epoch 384/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3256\n",
            "Epoch 385/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3329\n",
            "Epoch 386/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3330\n",
            "Epoch 387/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3276\n",
            "Epoch 388/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3218\n",
            "Epoch 389/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3398\n",
            "Epoch 390/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3331\n",
            "Epoch 391/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3348\n",
            "Epoch 392/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3316\n",
            "Epoch 393/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3302\n",
            "Epoch 394/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3279\n",
            "Epoch 395/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3236\n",
            "Epoch 396/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3293\n",
            "Epoch 397/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3331\n",
            "Epoch 398/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3257\n",
            "Epoch 399/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3276\n",
            "Epoch 400/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3225\n",
            "Epoch 401/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3293\n",
            "Epoch 402/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3177\n",
            "Epoch 403/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3275\n",
            "Epoch 404/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3146\n",
            "Epoch 405/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3259\n",
            "Epoch 406/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3252\n",
            "Epoch 407/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3245\n",
            "Epoch 408/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3238\n",
            "Epoch 409/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3309\n",
            "Epoch 410/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3304\n",
            "Epoch 411/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3298\n",
            "Epoch 412/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3243\n",
            "Epoch 413/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3261\n",
            "Epoch 414/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3280\n",
            "Epoch 415/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3343\n",
            "Epoch 416/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3222\n",
            "Epoch 417/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3204\n",
            "Epoch 418/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3202\n",
            "Epoch 419/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3251\n",
            "Epoch 420/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3211\n",
            "Epoch 421/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3222\n",
            "Epoch 422/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3217\n",
            "Epoch 423/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3164\n",
            "Epoch 424/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3218\n",
            "Epoch 425/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3229\n",
            "Epoch 426/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3184\n",
            "Epoch 427/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3146\n",
            "Epoch 428/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3190\n",
            "Epoch 429/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3218\n",
            "Epoch 430/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3330\n",
            "Epoch 431/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3250\n",
            "Epoch 432/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3213\n",
            "Epoch 433/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3167\n",
            "Epoch 434/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3108\n",
            "Epoch 435/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3202\n",
            "Epoch 436/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3199\n",
            "Epoch 437/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3206\n",
            "Epoch 438/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3123\n",
            "Epoch 439/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3183\n",
            "Epoch 440/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3147\n",
            "Epoch 441/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3148\n",
            "Epoch 442/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3094\n",
            "Epoch 443/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3198\n",
            "Epoch 444/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3132\n",
            "Epoch 445/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3200\n",
            "Epoch 446/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3164\n",
            "Epoch 447/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3204\n",
            "Epoch 448/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3153\n",
            "Epoch 449/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3148\n",
            "Epoch 450/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3146\n",
            "Epoch 451/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3130\n",
            "Epoch 452/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3092\n",
            "Epoch 453/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3123\n",
            "Epoch 454/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3150\n",
            "Epoch 455/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3146\n",
            "Epoch 456/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3086\n",
            "Epoch 457/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3105\n",
            "Epoch 458/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3074\n",
            "Epoch 459/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3100\n",
            "Epoch 460/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3116\n",
            "Epoch 461/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3174\n",
            "Epoch 462/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3332\n",
            "Epoch 463/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3297\n",
            "Epoch 464/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3169\n",
            "Epoch 465/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3101\n",
            "Epoch 466/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3099\n",
            "Epoch 467/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3143\n",
            "Epoch 468/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3169\n",
            "Epoch 469/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3182\n",
            "Epoch 470/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3110\n",
            "Epoch 471/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3124\n",
            "Epoch 472/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3101\n",
            "Epoch 473/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3041\n",
            "Epoch 474/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3113\n",
            "Epoch 475/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3081\n",
            "Epoch 476/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3182\n",
            "Epoch 477/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3080\n",
            "Epoch 478/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3122\n",
            "Epoch 479/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3083\n",
            "Epoch 480/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3120\n",
            "Epoch 481/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3048\n",
            "Epoch 482/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3078\n",
            "Epoch 483/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3147\n",
            "Epoch 484/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3064\n",
            "Epoch 485/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3082\n",
            "Epoch 486/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3138\n",
            "Epoch 487/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3090\n",
            "Epoch 488/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3147\n",
            "Epoch 489/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3092\n",
            "Epoch 490/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3075\n",
            "Epoch 491/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3155\n",
            "Epoch 492/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3039\n",
            "Epoch 493/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3056\n",
            "Epoch 494/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3119\n",
            "Epoch 495/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3105\n",
            "Epoch 496/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3163\n",
            "Epoch 497/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3169\n",
            "Epoch 498/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3078\n",
            "Epoch 499/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3042\n",
            "Epoch 500/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3071\n",
            "Epoch 501/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3088\n",
            "Epoch 502/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3034\n",
            "Epoch 503/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3026\n",
            "Epoch 504/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3109\n",
            "Epoch 505/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3048\n",
            "Epoch 506/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3101\n",
            "Epoch 507/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3097\n",
            "Epoch 508/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3117\n",
            "Epoch 509/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3085\n",
            "Epoch 510/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3159\n",
            "Epoch 511/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3169\n",
            "Epoch 512/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3065\n",
            "Epoch 513/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3106\n",
            "Epoch 514/800\n",
            "142/142 [==============================] - 1s 6ms/step - loss: 0.3028\n",
            "Epoch 515/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3126\n",
            "Epoch 516/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3100\n",
            "Epoch 517/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3005\n",
            "Epoch 518/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3014\n",
            "Epoch 519/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3029\n",
            "Epoch 520/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3048\n",
            "Epoch 521/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3051\n",
            "Epoch 522/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3039\n",
            "Epoch 523/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3081\n",
            "Epoch 524/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3018\n",
            "Epoch 525/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3217\n",
            "Epoch 526/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3158\n",
            "Epoch 527/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3192\n",
            "Epoch 528/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3051\n",
            "Epoch 529/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3009\n",
            "Epoch 530/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3029\n",
            "Epoch 531/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3033\n",
            "Epoch 532/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3045\n",
            "Epoch 533/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2977\n",
            "Epoch 534/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3064\n",
            "Epoch 535/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2985\n",
            "Epoch 536/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3042\n",
            "Epoch 537/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3045\n",
            "Epoch 538/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3065\n",
            "Epoch 539/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3056\n",
            "Epoch 540/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3059\n",
            "Epoch 541/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3011\n",
            "Epoch 542/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3066\n",
            "Epoch 543/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3034\n",
            "Epoch 544/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3004\n",
            "Epoch 545/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3018\n",
            "Epoch 546/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3107\n",
            "Epoch 547/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3104\n",
            "Epoch 548/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3063\n",
            "Epoch 549/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3003\n",
            "Epoch 550/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3002\n",
            "Epoch 551/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3028\n",
            "Epoch 552/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3022\n",
            "Epoch 553/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2978\n",
            "Epoch 554/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2990\n",
            "Epoch 555/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2963\n",
            "Epoch 556/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3050\n",
            "Epoch 557/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3000\n",
            "Epoch 558/800\n",
            "142/142 [==============================] - 1s 6ms/step - loss: 0.3033\n",
            "Epoch 559/800\n",
            "142/142 [==============================] - 1s 6ms/step - loss: 0.3022\n",
            "Epoch 560/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3096\n",
            "Epoch 561/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3070\n",
            "Epoch 562/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3069\n",
            "Epoch 563/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3103\n",
            "Epoch 564/800\n",
            "142/142 [==============================] - 1s 6ms/step - loss: 0.3018\n",
            "Epoch 565/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3035\n",
            "Epoch 566/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3004\n",
            "Epoch 567/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2981\n",
            "Epoch 568/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3005\n",
            "Epoch 569/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3001\n",
            "Epoch 570/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3016\n",
            "Epoch 571/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3041\n",
            "Epoch 572/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3018\n",
            "Epoch 573/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3024\n",
            "Epoch 574/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3048\n",
            "Epoch 575/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2971\n",
            "Epoch 576/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3039\n",
            "Epoch 577/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2992\n",
            "Epoch 578/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2999\n",
            "Epoch 579/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3016\n",
            "Epoch 580/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3013\n",
            "Epoch 581/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2991\n",
            "Epoch 582/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3005\n",
            "Epoch 583/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2955\n",
            "Epoch 584/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2995\n",
            "Epoch 585/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2987\n",
            "Epoch 586/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3029\n",
            "Epoch 587/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2988\n",
            "Epoch 588/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3082\n",
            "Epoch 589/800\n",
            "142/142 [==============================] - 1s 6ms/step - loss: 0.2982\n",
            "Epoch 590/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2993\n",
            "Epoch 591/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3055\n",
            "Epoch 592/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3046\n",
            "Epoch 593/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2980\n",
            "Epoch 594/800\n",
            "142/142 [==============================] - 1s 6ms/step - loss: 0.2960\n",
            "Epoch 595/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2963\n",
            "Epoch 596/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2975\n",
            "Epoch 597/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2965\n",
            "Epoch 598/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2942\n",
            "Epoch 599/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2976\n",
            "Epoch 600/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3024\n",
            "Epoch 601/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2975\n",
            "Epoch 602/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2966\n",
            "Epoch 603/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3010\n",
            "Epoch 604/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3010\n",
            "Epoch 605/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3029\n",
            "Epoch 606/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2937\n",
            "Epoch 607/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2955\n",
            "Epoch 608/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2979\n",
            "Epoch 609/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2978\n",
            "Epoch 610/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2996\n",
            "Epoch 611/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3002\n",
            "Epoch 612/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3009\n",
            "Epoch 613/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3009\n",
            "Epoch 614/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2968\n",
            "Epoch 615/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2938\n",
            "Epoch 616/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2985\n",
            "Epoch 617/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3031\n",
            "Epoch 618/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2993\n",
            "Epoch 619/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3004\n",
            "Epoch 620/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3012\n",
            "Epoch 621/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2949\n",
            "Epoch 622/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2958\n",
            "Epoch 623/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2976\n",
            "Epoch 624/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2966\n",
            "Epoch 625/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3018\n",
            "Epoch 626/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2983\n",
            "Epoch 627/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2971\n",
            "Epoch 628/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2935\n",
            "Epoch 629/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2968\n",
            "Epoch 630/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2988\n",
            "Epoch 631/800\n",
            "142/142 [==============================] - 1s 6ms/step - loss: 0.2998\n",
            "Epoch 632/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2911\n",
            "Epoch 633/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2967\n",
            "Epoch 634/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2952\n",
            "Epoch 635/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2943\n",
            "Epoch 636/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2937\n",
            "Epoch 637/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2968\n",
            "Epoch 638/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2932\n",
            "Epoch 639/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2979\n",
            "Epoch 640/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2993\n",
            "Epoch 641/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2909\n",
            "Epoch 642/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2963\n",
            "Epoch 643/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2934\n",
            "Epoch 644/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2961\n",
            "Epoch 645/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3006\n",
            "Epoch 646/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2939\n",
            "Epoch 647/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2905\n",
            "Epoch 648/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2929\n",
            "Epoch 649/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2956\n",
            "Epoch 650/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2908\n",
            "Epoch 651/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2989\n",
            "Epoch 652/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2963\n",
            "Epoch 653/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2989\n",
            "Epoch 654/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2941\n",
            "Epoch 655/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2976\n",
            "Epoch 656/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2974\n",
            "Epoch 657/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2922\n",
            "Epoch 658/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2960\n",
            "Epoch 659/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3003\n",
            "Epoch 660/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2960\n",
            "Epoch 661/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2980\n",
            "Epoch 662/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2943\n",
            "Epoch 663/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2921\n",
            "Epoch 664/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2892\n",
            "Epoch 665/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2932\n",
            "Epoch 666/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2999\n",
            "Epoch 667/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2968\n",
            "Epoch 668/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2906\n",
            "Epoch 669/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2971\n",
            "Epoch 670/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2942\n",
            "Epoch 671/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2930\n",
            "Epoch 672/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2910\n",
            "Epoch 673/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2947\n",
            "Epoch 674/800\n",
            "142/142 [==============================] - 1s 6ms/step - loss: 0.2938\n",
            "Epoch 675/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2894\n",
            "Epoch 676/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2944\n",
            "Epoch 677/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2967\n",
            "Epoch 678/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2988\n",
            "Epoch 679/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2908\n",
            "Epoch 680/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2965\n",
            "Epoch 681/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2932\n",
            "Epoch 682/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2939\n",
            "Epoch 683/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2929\n",
            "Epoch 684/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2898\n",
            "Epoch 685/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2978\n",
            "Epoch 686/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2911\n",
            "Epoch 687/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2926\n",
            "Epoch 688/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2900\n",
            "Epoch 689/800\n",
            "142/142 [==============================] - 1s 6ms/step - loss: 0.2886\n",
            "Epoch 690/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2947\n",
            "Epoch 691/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2903\n",
            "Epoch 692/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2951\n",
            "Epoch 693/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2996\n",
            "Epoch 694/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2990\n",
            "Epoch 695/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2936\n",
            "Epoch 696/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2937\n",
            "Epoch 697/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2925\n",
            "Epoch 698/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2904\n",
            "Epoch 699/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2929\n",
            "Epoch 700/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2860\n",
            "Epoch 701/800\n",
            "142/142 [==============================] - 1s 6ms/step - loss: 0.2908\n",
            "Epoch 702/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2892\n",
            "Epoch 703/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2842\n",
            "Epoch 704/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2888\n",
            "Epoch 705/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2907\n",
            "Epoch 706/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2878\n",
            "Epoch 707/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2909\n",
            "Epoch 708/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2938\n",
            "Epoch 709/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2899\n",
            "Epoch 710/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2926\n",
            "Epoch 711/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2879\n",
            "Epoch 712/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2911\n",
            "Epoch 713/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2932\n",
            "Epoch 714/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2903\n",
            "Epoch 715/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2892\n",
            "Epoch 716/800\n",
            "142/142 [==============================] - 1s 6ms/step - loss: 0.2905\n",
            "Epoch 717/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2941\n",
            "Epoch 718/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2957\n",
            "Epoch 719/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2932\n",
            "Epoch 720/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2894\n",
            "Epoch 721/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2881\n",
            "Epoch 722/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2905\n",
            "Epoch 723/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2924\n",
            "Epoch 724/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2869\n",
            "Epoch 725/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2909\n",
            "Epoch 726/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2905\n",
            "Epoch 727/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2870\n",
            "Epoch 728/800\n",
            "142/142 [==============================] - 1s 6ms/step - loss: 0.2875\n",
            "Epoch 729/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2839\n",
            "Epoch 730/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2948\n",
            "Epoch 731/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.3006\n",
            "Epoch 732/800\n",
            "142/142 [==============================] - 1s 6ms/step - loss: 0.2953\n",
            "Epoch 733/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2948\n",
            "Epoch 734/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2991\n",
            "Epoch 735/800\n",
            "142/142 [==============================] - 1s 6ms/step - loss: 0.2902\n",
            "Epoch 736/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2955\n",
            "Epoch 737/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2888\n",
            "Epoch 738/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2917\n",
            "Epoch 739/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2815\n",
            "Epoch 740/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2885\n",
            "Epoch 741/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2924\n",
            "Epoch 742/800\n",
            "142/142 [==============================] - 1s 6ms/step - loss: 0.2928\n",
            "Epoch 743/800\n",
            "142/142 [==============================] - 1s 6ms/step - loss: 0.2897\n",
            "Epoch 744/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2953\n",
            "Epoch 745/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2980\n",
            "Epoch 746/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2893\n",
            "Epoch 747/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2872\n",
            "Epoch 748/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2876\n",
            "Epoch 749/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2924\n",
            "Epoch 750/800\n",
            "142/142 [==============================] - 1s 6ms/step - loss: 0.2829\n",
            "Epoch 751/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2915\n",
            "Epoch 752/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2835\n",
            "Epoch 753/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2874\n",
            "Epoch 754/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2893\n",
            "Epoch 755/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2898\n",
            "Epoch 756/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2893\n",
            "Epoch 757/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2922\n",
            "Epoch 758/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2857\n",
            "Epoch 759/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2912\n",
            "Epoch 760/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2907\n",
            "Epoch 761/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2959\n",
            "Epoch 762/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2893\n",
            "Epoch 763/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2926\n",
            "Epoch 764/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2876\n",
            "Epoch 765/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2892\n",
            "Epoch 766/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2876\n",
            "Epoch 767/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2880\n",
            "Epoch 768/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2869\n",
            "Epoch 769/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2888\n",
            "Epoch 770/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2901\n",
            "Epoch 771/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2800\n",
            "Epoch 772/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2880\n",
            "Epoch 773/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2960\n",
            "Epoch 774/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2894\n",
            "Epoch 775/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2907\n",
            "Epoch 776/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2904\n",
            "Epoch 777/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2859\n",
            "Epoch 778/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2869\n",
            "Epoch 779/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2779\n",
            "Epoch 780/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2822\n",
            "Epoch 781/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2829\n",
            "Epoch 782/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2956\n",
            "Epoch 783/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2891\n",
            "Epoch 784/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2906\n",
            "Epoch 785/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2893\n",
            "Epoch 786/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2877\n",
            "Epoch 787/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2918\n",
            "Epoch 788/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2832\n",
            "Epoch 789/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2903\n",
            "Epoch 790/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2874\n",
            "Epoch 791/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2850\n",
            "Epoch 792/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2850\n",
            "Epoch 793/800\n",
            "142/142 [==============================] - 1s 6ms/step - loss: 0.2915\n",
            "Epoch 794/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2898\n",
            "Epoch 795/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2903\n",
            "Epoch 796/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2865\n",
            "Epoch 797/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2859\n",
            "Epoch 798/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2841\n",
            "Epoch 799/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2809\n",
            "Epoch 800/800\n",
            "142/142 [==============================] - 1s 5ms/step - loss: 0.2872\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> **Text Generation (Prediction)**\n",
        "\n"
      ],
      "metadata": {
        "id": "Oce8kfPCpZin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(seed_text,next_words,model,max_sequence_len):\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list],maxlen=max_sequence_len-1,padding='pre')\n",
        "        predict_x=model.predict(token_list,verbose=0) \n",
        "        classes_x=np.argmax(predict_x,axis=1)\n",
        "        output_word = \"\"\n",
        "        for word,index in tokenizer.word_index.items():\n",
        "            if index==classes_x:\n",
        "                output_word = word\n",
        "                break\n",
        "        seed_text += \" \"+ output_word\n",
        "    return seed_text.title()"
      ],
      "metadata": {
        "id": "40VAjPS-pRNC"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "> Prediction\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SQxtxlc3gp_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(\"Political\",8,model,max_sequence_len))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcgHvxzJtN64",
        "outputId": "b9b0d323-8aee-496b-d25b-6dbbe707ed01"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Political Support Divides Art Therapists The Same Friend Freely\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (generate_text(\"president trump\", 3, model, max_sequence_len))\n",
        "print (generate_text(\"united states\", 4, model, max_sequence_len))\n",
        "print (generate_text(\"donald trump\", 2, model, max_sequence_len))\n",
        "print (generate_text(\"new york\", 3, model, max_sequence_len))\n",
        "print (generate_text(\"science and technology\", 5, model, max_sequence_len))"
      ],
      "metadata": {
        "id": "B_FeIiWjtR2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ee8c6e5-1a21-4464-a35f-570bb49d7997"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "President Trump And Staff Rethink\n",
            "United States Race To Pillars Of\n",
            "Donald Trump Master Of\n",
            "New York Today A Goodbye\n",
            "Science And Technology Rules Of The Game Era\n"
          ]
        }
      ]
    }
  ]
}